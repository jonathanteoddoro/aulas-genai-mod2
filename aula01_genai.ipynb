{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbO6dLRFbzOY"
      },
      "source": [
        "# **Inteligência Artificial Generativa**\n",
        "A **Inteligência Artificial Generativa** é uma categoria de sistemas de IA projetada para criar conteúdo original, diferentemente dos sistemas tradicionais que apenas analisam ou classificam dados existentes. Esses modelos podem produzir textos, imagens, áudios, vídeos e código de programação completamente novos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk-wSks2dF0C"
      },
      "source": [
        "## **Como funciona a geração?**\n",
        "\n",
        "O funcionamento desses sistemas tem como base **redes neurais profundas**, especialmente a arquitetura **Transformer**, que revolucionou o processamento de sequências por sua eficiência em capturar dependências complexas entre elementos.\n",
        "\n",
        "Nos **Modelos de Linguagem de Grande Escala (LLMs)**, o processo de geração é essencialmente uma **predição sequencial**. O modelo não “sabe” a resposta completa de imediato — ele constrói a saída passo a passo.\n",
        "\n",
        "Quando recebe um texto de entrada (**prompt**), o modelo realiza os seguintes passos:\n",
        "\n",
        "1. **Tokenização da entrada** – O texto é convertido em tokens (unidades numéricas).\n",
        "2. **Cálculo das distribuições de probabilidade** – Para cada posição da sequência, a rede estima a probabilidade de cada token possível ser o próximo.\n",
        "3. **Seleção do próximo token** – O modelo escolhe o token mais adequado com base nessa distribuição, de acordo com a configuração do parâmetro de **temperatura**, que regula o nível de aleatoriedade da escolha.\n",
        "4. **Iteração do processo** – O token selecionado é adicionado à sequência, e o cálculo é repetido até que a resposta esteja completa ou um limite seja atingido.\n",
        "\n",
        "Essa capacidade de prever a próxima unidade só é possível porque, durante o treinamento, o modelo aprende **representações vetoriais densas (embeddings)** que capturam padrões estatísticos, semânticos e sintáticos do texto. Assim, cada escolha feita pelo modelo leva em consideração o **contexto completo da entrada**, e não apenas os tokens imediatamente anteriores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRFkdBfkdKZm"
      },
      "source": [
        "## **Processo de Treinamento**\n",
        "\n",
        "O treinamento de modelos generativos ocorre em **duas fases principais**, que se complementam e permitem ao sistema tanto adquirir conhecimento geral quanto alinhar seu comportamento a objetivos específicos.\n",
        "\n",
        "1. **Pré-treinamento**\n",
        "   Nesta fase, o modelo é exposto a grandes volumes de texto, geralmente coletados de diversas fontes. A tarefa principal é **prever o próximo token em uma sequência**.\n",
        "\n",
        "   * Essa tarefa, apesar de simples, força a rede a aprender padrões de linguagem, relações semânticas e sintáticas, além de acumular conhecimento factual e contextual.\n",
        "   * O pré-treinamento é intensivo em termos de **dados** e **poder computacional**, resultando em um modelo capaz de generalizar sobre uma ampla variedade de contextos.\n",
        "\n",
        "2. **Ajuste fino (fine-tuning)**\n",
        "   Após o pré-treinamento, o modelo é adaptado para responder de forma mais útil e alinhada às expectativas humanas.\n",
        "\n",
        "   * O ajuste fino pode utilizar **conjuntos de dados anotados** com exemplos de perguntas e respostas desejadas, o que direciona o comportamento do modelo.\n",
        "   * Uma técnica central nesse processo é o **Aprendizado por Reforço com Feedback Humano (RLHF)**. Nesse método, avaliadores humanos classificam as respostas geradas, e o modelo é otimizado para preferir saídas que reflitam essas classificações.\n",
        "   * Esse mecanismo ajusta o modelo não apenas para a correção linguística, mas também para critérios como **clareza, utilidade, segurança e alinhamento com valores humanos**.\n",
        "\n",
        "O resultado da combinação entre pré-treinamento e ajuste fino é um sistema robusto: capaz de **compreender e produzir linguagem natural em múltiplos contextos**, além de responder de forma mais controlada, confiável e útil."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL4kUEVIdNN-"
      },
      "source": [
        "## **Conceitos Fundamentais**\n",
        "\n",
        "**Tokens e tokenização** formam a base do processamento. O texto não é tratado como palavras inteiras, mas fragmentado em unidades menores chamadas tokens - que podem ser palavras completas, subpalavras ou caracteres. A tokenização converte o texto em uma sequência numérica que o modelo consegue processar.\n",
        "\n",
        "**Embeddings** são representações vetoriais densas de cada token. Tokens com significados semelhantes ocupam posições próximas no espaço vetorial, permitindo que o modelo compreenda relações entre conceitos, sinônimos e contextos diferentes. Essa proximidade matemática reflete proximidade semântica.\n",
        "\n",
        "**Atenção e auto-atenção** representam o mecanismo central da arquitetura Transformer. O sistema de atenção identifica quais partes da entrada são mais relevantes para gerar cada elemento da saída. Na auto-atenção, cada token examina simultaneamente todos os outros tokens da sequência, capturando dependências de curto e longo alcance.\n",
        "\n",
        "**Parâmetros** são os pesos ajustáveis da rede neural que armazenam o conhecimento do modelo. Sistemas modernos possuem bilhões de parâmetros, permitindo que armazenem e manipulem conhecimento extenso sobre linguagem e conceitos diversos.\n",
        "\n",
        "**Temperatura** controla a aleatoriedade da geração. Valores baixos (0.1-0.3) produzem saídas mais determinísticas e conservadoras, enquanto valores altos (0.7-1.0) geram respostas mais criativas e variadas. É um hiperparâmetro que permite ajustar o equilíbrio entre precisão e criatividade conforme a necessidade.\n",
        "\n",
        "Esses elementos trabalham em conjunto para criar sistemas capazes de gerar conteúdo original e contextualmente relevante, representando um avanço significativo em relação aos modelos de IA anteriores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpg2mZ76dQty"
      },
      "source": [
        "## **Exemplo de uso da API da OpenAI**\n",
        "\n",
        "Para interagir com modelos de **Inteligência Artificial Generativa**, como o **GPT**, podemos utilizar a API da OpenAI. Isso permite enviar instruções (chamadas de *prompts*) para o modelo e receber respostas de forma programática, integrando a IA em aplicações, sistemas e fluxos de trabalho.\n",
        "\n",
        "No exemplo abaixo, utilizamos a biblioteca oficial `openai` em Python para fazer uma chamada simples a um modelo de linguagem:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5EmkMZybwqL",
        "outputId": "21b66f98-4136-435d-94b6-14510c658f9a"
      },
      "outputs": [],
      "source": [
        "%pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "35WJpkftbuuI",
        "outputId": "361046e6-4141-4300-bc3d-776982fb1daa"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"SUA_API_KEY_AQUI\")\n",
        "\n",
        "resposta = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Me explique o que é inteligência artificial.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(resposta.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpOfiWChdleb"
      },
      "source": [
        "\n",
        "### **Como funciona o código?**\n",
        "\n",
        "1. **Importação e autenticação** – A classe `OpenAI` é inicializada com a chave de API, que autentica o acesso ao serviço.\n",
        "2. **Envio da requisição** – Chamamos `chat.completions.create` passando:\n",
        "\n",
        "   * O modelo desejado (`gpt-4o-mini`, neste caso).\n",
        "   * Uma lista de mensagens, onde cada mensagem possui um papel (`role`) e um conteúdo (`content`).\n",
        "3. **Recebimento da resposta** – O modelo processa a entrada e retorna uma resposta, que pode ser acessada em `resposta.choices[0].message.content`.\n",
        "\n",
        "Esse fluxo demonstra de forma simples como integrar um modelo generativo em uma aplicação. A partir dessa base, é possível construir soluções mais elaboradas, como **chatbots, assistentes virtuais, ferramentas de apoio à escrita, sistemas de análise de dados** e muito mais.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
